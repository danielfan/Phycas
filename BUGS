This document describes some of the nastier bugs (ones that have taken several days to figure out).
The hope is that if these are recorded somewhere, then history will not repeat itself!

Bug 1: the "total_count" bug (fixed in revision 292 on 3-Nov-2006)
------------------------------------------------------------------
In the Gelfand-Ghosh calculations, a map in which patterns are keys and pattern counts are
values is generated that maintains the sum of all posterior predictive datasets generated
during the run. At the end of the run, each count is divided by the number of data sets added,
so that this map, called gg_mu, ends up being best described as the "average" posterior predictive
data set. The problem here was that the counts were stored as floats, and adding enough of these
together finally overflows the float value, creating errors in the total number of counts over
all patterns. The main symptom was that adding a value such as 1 to total_count yielded an updated
total_count that was not 1 more than its previous value! The example below explains why this 
happens. This bug mostly affected the SimData class (files sim_data.hpp, sim_data.inl and sim_data.cpp).

Here is an example of how the overflow is manifested. A count of 1139.0 is added to the existing
total count 16776298.0, which should yield 16777437.0, but instead yields 16777436.0.

total_count before:  16776298.0 is  111111111111110001101010 in binary
added amount:            1139.0 is               10001110011 in binary
----------------------------------------------------------------------
total_count after should be:       1000000000000000011011101 in binary

Single precision floats are stored as a 23 bit mantissa and an 8 bit biased exponent, plus a 
sign bit. For example, to represent 1139.0, start with the binary representation, 10001110011,
and move the decimal point left until achieving the normalized representation

 10001110011 = 1.0001110011 * 2^{10}
 
Adding the bias (127) to the exponent (biasing is necessary to allow negative exponents to be 
represented as unsigned values) yields 127 + 10 = 137, which is 10001001 binary. Thus, the
sign bit is 0 (because the number is positive), the 8 bit exponent is 10001001 and the 23 bit
mantissa (padded on the right with 13 0s) is 00011100110000000000000:

 1139 = 0|10001001|00011100110000000000000
 
Doing the same with total_count after the addtion, we have 
 
 16777437.0 = 1.000000000000000011011101 * 2^{24} = 0|10010111|000000000000000011011101
 
Unfortunately, the mantissa is 24 bits long rather than the 23 allowed, so the trailing
1 gets lopped off, giving us the following instead:

 16777436.0 = 0|10010111|00000000000000001101110

This is the source of the confusion surrounding this bug. The value of total_count was 
accurate until enough simulated datasets had been added to gg_mu to cause overflow of this
sort, at which point adding 1 didn't necessarily increase total_count by 1! 

Solution:

The best solution seems to keep the counts as floats, but instead of waiting until all
posterior predictive data sets have been added to gg_mu to before doing the division, keep
a running average so that the total number of counts does not grow too large.

For example, suppose there are only four possible patterns, and the following pattern counts 
are generated by performing three posterior predictive simulations:

[213, 287,   0,   0]
[177,   0, 323,   0]
[103,   0,  97, 300]

In each case, the sum of all pattern counts is 500, so the total count is 1500 after all 
three have been added. The strategy before would be to add values for each pattern over
all three replicates, then divide by 3 at the end, so mu vector would be:

[164.333, 95.667, 140, 100]

The alternative is to add each pair in turn to a running average as follows:

Add first posterior predictive replicate (r1) to mu:
p = 1/1 (p is always 1/(number of posterior predictive datasets added)
mu = [ 0,     0, 0, 0 ] =>   [ 0*(1-p), 0*(1-p), 0*(1-p), 0*(1-p) ]
r1 = [ 213, 287, 0, 0 ] => + [   213*p,   287*p,     0*p,     0*p ]
                    new mu = [     213,     287,       0,       0 ]

Add second posterior predictive replicate (r2) to mu:
p = 1/2
mu = [ 213, 287,   0, 0 ] =>   [ 213*(1-p), 287*(1-p), 0*(1-p), 0*(1-p) ]
r2 = [ 177,   0, 323, 0 ] => + [     177*p,       0*p,   323*p,     0*p ]
                      new mu = [       195,     143.5,   161.5,       0 ]

Add third posterior predictive replicate (r3) to mu:
p = 1/3
mu = [ 195, 143.5, 161.5,   0 ] =>   [ 195*(1-p), 143.5*(1-p), 161.5*(1-p), 0*(1-p) ]
r3 = [ 103,     0,    97, 300 ] => + [     103*p,         0*p,        97*p,   300*p ]
                            new mu = [   164.333,      95.667,         140,     100 ]

To implement the above averaging scheme required chainging only four lines, all 
in Phycas.py. These three lines were added just before the contents of tmp_simdata
are added to gg_mu in the recordSample function:

  p = 1.0/self.gg_num_post_pred_reps
  self.gg_mu.multBy(1.0 - p)
  self.tmp_simdata.multBy(p)

The following line, which formerly did the final division, was removed from the 
run function:

  self.gg_mu.divideBy(float(self.gg_total)
