This file documents major bugs in Phycas. A major bug is loosely defined as something
that prevents a user from using Phycas to analyze a particular data set, or something
that causes Phycas to generate incorrect results. Major bugs usually take a day or more
to fix, and documenting them helps to avoid repeating history. Minor bugs represent 
cosmetic glitches that are easy to work around and do not take long to fix. Minor bugs 
are best documented in SVN comments.

Bug 2: the "topology prior table" bug (fixed in revision 421 on 13-Aug-2007)
----------------------------------------------------------------------------
This bug was first reported by Stephano Mona on 21 July 2007.
Using the polytomy model with a data set of 186 taxa resulted in this error:

Topology prior:
  Prior type:
polytomy prior
  Prior strength (C): 2.71828182846
  Expected prior probability for each resolution class:
   class        prior
  -------------------
       1      0.00000
       2      0.00000
...
      38      0.00000
      39      0.00000
Traceback (most recent call last):
  File "mona.py", line 89, in <module>
    phycas.run()
  File "C:\Synchronized\Projects\phycasdev_trunk\phycas\Phycas\Phycas.py", line 935, in run
    self.showTopoPriorInfo()
  File "C:\Synchronized\Projects\phycasdev_trunk\phycas\Phycas\Phycas.py", line 475, in showTopoPriorInfo
    self.output('%8d %12.5f' % (i,math.exp(v - denom)))
ValueError: math domain error

Resolution:
-----------
The problem was in the display of the realized prior probabilities for topologies. The numerator 
of each of these probabilities can get large because it involves the number of tree topologies 
for a given number of internal nodes. For this problem (186 taxa), the numerator overflowed for 
trees having more than 39 internal nodes and the crash occurred when it tried to subtract 
1.#INF from 1.#INF.

Fixing the bug required reworking parts of the TopoPriorCalculator class 
(phycas/src/topo_prior_calculator.cpp). In particular, the RecalcCountsAndPriorsImpl function was
reworked so that the vector counts is now associated with a vector nfactors, the ith element of 
which contains the number of times a factor needed to be removed in order to keep the magnitude
of counts[i] reasonable. The factor taken out is scaling_factor (saved as the log in a new variable
named log_scaling_factor). Thus, if scaling_factor = 10, nfactors[i] = 3, and counts[i] = 1.5, the 
value of the log_scaling_factor variable would be log(10) and the actual count would be 1.5*10^3.
Even with this system, the log of the total topology count is still difficult to obtain because
each log count must be exponentiated in order to add it to the sum. Thus, the largest log count is
determined and factored out of the sum, then its log is added back in at the end. Many of the 
smaller counts will disappear, but they are negligible anyway given that the total is never used 
in any MCMC calculations. A similar factoring out of the largest value was required in the function 
GetRealizedResClassPriorsVect. Now, a valid table of resolution class priors is presented to the
user prior to an analysis involving polytomies. For large numbers of taxa (e.g. 186), many of the
values will appear to be zero. A note has been added to the output apprising users of the fact that
even though probabilities are reported as 0.00000000, they are not in fact exactly zero and this
result simply means that the value is less than 0.000000005 and rounds off to zero when only 8
decimal places are used.

Bug 1: the "total_count" bug (fixed in revision 292 on 3-Nov-2006)
------------------------------------------------------------------
In the Gelfand-Ghosh calculations, a map in which patterns are keys and pattern counts are
values is generated that maintains the sum of all posterior predictive datasets generated
during the run. At the end of the run, each count is divided by the number of data sets added,
so that this map, called gg_mu, ends up being best described as the "average" posterior predictive
data set. The problem here was that the counts were stored as floats, and adding enough of these
together finally overflows the float value, creating errors in the total number of counts over
all patterns. The main symptom was that adding a value such as 1 to total_count yielded an updated
total_count that was not 1 more than its previous value! The example below explains why this 
happens. This bug mostly affected the SimData class (files sim_data.hpp, sim_data.inl and sim_data.cpp).

Here is an example of how the overflow is manifested. A count of 1139.0 is added to the existing
total count 16776298.0, which should yield 16777437.0, but instead yields 16777436.0.

total_count before:  16776298.0 is  111111111111110001101010 in binary
added amount:            1139.0 is               10001110011 in binary
----------------------------------------------------------------------
total_count after should be:       1000000000000000011011101 in binary

Single precision floats are stored as a 23 bit mantissa and an 8 bit biased exponent, plus a 
sign bit. For example, to represent 1139.0, start with the binary representation, 10001110011,
and move the decimal point left until achieving the normalized representation

 10001110011 = 1.0001110011 * 2^{10}
 
Adding the bias (127) to the exponent (biasing is necessary to allow negative exponents to be 
represented as unsigned values) yields 127 + 10 = 137, which is 10001001 binary. Thus, the
sign bit is 0 (because the number is positive), the 8 bit exponent is 10001001 and the 23 bit
mantissa (padded on the right with 13 0s) is 00011100110000000000000:

 1139 = 0|10001001|00011100110000000000000
 
Doing the same with total_count after the addtion, we have 
 
 16777437.0 = 1.000000000000000011011101 * 2^{24} = 0|10010111|000000000000000011011101
 
Unfortunately, the mantissa is 24 bits long rather than the 23 allowed, so the trailing
1 gets lopped off, giving us the following instead:

 16777436.0 = 0|10010111|00000000000000001101110

This is the source of the confusion surrounding this bug. The value of total_count was 
accurate until enough simulated datasets had been added to gg_mu to cause overflow of this
sort, at which point adding 1 didn't necessarily increase total_count by 1! 

Solution:

The best solution seems to keep the counts as floats, but instead of waiting until all
posterior predictive data sets have been added to gg_mu to before doing the division, keep
a running average so that the total number of counts does not grow too large.

For example, suppose there are only four possible patterns, and the following pattern counts 
are generated by performing three posterior predictive simulations:

[213, 287,   0,   0]
[177,   0, 323,   0]
[103,   0,  97, 300]

In each case, the sum of all pattern counts is 500, so the total count is 1500 after all 
three have been added. The strategy before would be to add values for each pattern over
all three replicates, then divide by 3 at the end, so mu vector would be:

[164.333, 95.667, 140, 100]

The alternative is to add each pair in turn to a running average as follows:

Add first posterior predictive replicate (r1) to mu:
p = 1/1 (p is always 1/(number of posterior predictive datasets added)
mu = [ 0,     0, 0, 0 ] =>   [ 0*(1-p), 0*(1-p), 0*(1-p), 0*(1-p) ]
r1 = [ 213, 287, 0, 0 ] => + [   213*p,   287*p,     0*p,     0*p ]
                    new mu = [     213,     287,       0,       0 ]

Add second posterior predictive replicate (r2) to mu:
p = 1/2
mu = [ 213, 287,   0, 0 ] =>   [ 213*(1-p), 287*(1-p), 0*(1-p), 0*(1-p) ]
r2 = [ 177,   0, 323, 0 ] => + [     177*p,       0*p,   323*p,     0*p ]
                      new mu = [       195,     143.5,   161.5,       0 ]

Add third posterior predictive replicate (r3) to mu:
p = 1/3
mu = [ 195, 143.5, 161.5,   0 ] =>   [ 195*(1-p), 143.5*(1-p), 161.5*(1-p), 0*(1-p) ]
r3 = [ 103,     0,    97, 300 ] => + [     103*p,         0*p,        97*p,   300*p ]
                            new mu = [   164.333,      95.667,         140,     100 ]

To implement the above averaging scheme required changing only four lines, all 
in Phycas.py. These three lines were added just before the contents of tmp_simdata
are added to gg_mu in the recordSample function:

  p = 1.0/self.gg_num_post_pred_reps
  self.gg_mu.multBy(1.0 - p)
  self.tmp_simdata.multBy(p)

The following line, which formerly did the final division, was removed from the 
run function:

  self.gg_mu.divideBy(float(self.gg_total)
