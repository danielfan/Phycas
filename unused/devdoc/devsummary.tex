\documentclass[10pt]{article}
\setlength{\oddsidemargin}{0.in}
\setlength{\textwidth}{6.5in}
\setlength{\topmargin}{-0.25in}
\setlength{\textheight}{8.25in}

% Skip space between paragraphs
\setlength{\parskip}{.1in}

% Do not put numbers in section headings
\setcounter{secnumdepth}{0}

% Indent paragraphs 0. in
\setlength{\parindent}{0.in}

% Use the natbib package for the bibliography
\usepackage[round]{natbib}
\bibliographystyle{sysbio}

% Use the graphicx package to incorporate and scale
% encapsulated postscript figures
\usepackage{graphicx}

% Make document single-spaced
\renewcommand{\baselinestretch}{1.0}

\newcommand{\svnurl}{https://svn.scs.fsu.edu/svn/phycasdev}
\newcommand{\None}{\verb+None+}
\newcommand{\new}{{\bfseries *** NEW ***}}

\begin{document}

\title{Current State of Phycas}
\author{Paul O. Lewis} 
\date{16 May 2006 (revised 13 June 2006)}
\maketitle

\section{Intended Audience}

This document was prepared for the purpose of bringing Phycas co-authors Mark T. Holder and David L. Swofford up to speed on Phycas development for the meeting at NESCENT 17-20 May, 2006. I have taken a first stab at creating a usable (if not user-friendly) Phycas library and application, but I'm open to any changes you (Mark and Dave) feel are important. Note that the relevant directory structure is listed at the end, so skip to the back at any point if you are lost as to where to find the files mentioned herein.

\section{Quick Start}

Phycas is currently both a C++/Python library as well as an application. Hereafter, the library will be referred to as PyPhy, whereas the application will be called simply Phycas. The Phycas application itself is represented by the file {\tt phycasdev/apps/Phycas/Phycas.py}, and the easiest way to run Phycas is to prepare a Python script that imports Phycas, resets a few variables, and then calls the \verb+Phycas.setup()+ and \verb+Phycas.run()+ functions to do the analysis. For example, here is a Python script that performs a Bayesian MCMC analysis using the HKY model:

\begin{verbatim}
from Phycas import *                     # import everything in Phycas.py

phycas = Phycas()                        # create a Phycas object

phycas.data_source = 'file'              # data will come from a nexus file
phycas.data_file_name = 'nyldna4.nex'    # here's the file name
phycas.starting_tree_source = 'random'   # starting tree will be generated randomly
phycas.ncycles = 1500                    # cycle through all parameters 1500 times
phycas.sample_every = 15                 # sample every 15th cycle
phycas.random_seed = '13579'             # use this master random number seed
phycas.model_type = 'hky'                # use the HKY model (as opposed to JC or GTR)
phycas.using_hyperprior = True           # use hierarchical model for edge length priors
phycas.verbose = True                    # show progress during MCMC

phycas.setup()                           # get ready for MCMC (build tree, read data, ...)
phycas.run()                             # do the MCMC analysis
\end{verbatim}

Other variables that can be changed before calling \verb+Phycas.setup()+ are inside \verb+Phycas.__init__()+ in {\tt Phycas.py}. Nearly all pyphy functionality can be invoked by simply changing the values of such variables before calling \verb+Phycas.setup()+.

Discussion point: should we allow users to put some or all of these Python commands inside a PHYCAS block in a NEXUS data file?

\section{Current Capabilities}

Phycas can currently conduct a single-chain MCMC analysis under the standard set of substitution models (i.e. JC, F81, K2P, HKY, GTR, plus I, G or I+G versions of these). It supports only a single partition at the moment, but support for data partitioning is the next major item on the TODO list. Phycas now supports {\bfseries several features not supported by MrBayes 3.1}, namely:

\begin{itemize}
\item Phycas uses slice sampling \citep{Neal2003a} for all individual updates of model parameters, which obviates the need for a plethora of tuning parameters and enables the automatic tuning during MCMC.
\item The option of using a hierarchical model for edge lengths in which the mean of the edge length prior is a hyperparameter that is itself updated during an MCMC analysis.
\item Reversible-jump MCMC to switch between fully-resolved and polytomous trees \citep{LewisHolderHolsinger2005}
\item The option of using the General Discrete Distribution for relative rates \citep{KosakovskypondMuse2005} and an associated reversible-jump MCMC move to switch between different numbers of rate categories.
\item Gelfand-Ghosh minimum posterior predictive loss approach to model selection, which can be automatically computed for any model implemented in Phycas
\item Being a Python library as well as an application, Phycas is much more scriptable and customizable than MrBayes. Users tempted to meddle can do a lot without having to touch a C or C++ compiler.
\end{itemize}

Despite these advantages, Phycas still has a long way to go. Here are some of its {\bfseries current limitations}:
\begin{itemize}
\item Slow. I have thus far concentrated on flexibility and ensuring that the likelihood calculations and MCMC moves are accurately implemented, but the calculation of likelihoods has not yet been optimized in any way. This is something I was hoping you guys could help me with while we are together this week. See below for actual speed comparison with MrBayes.
\item Underflow is not yet implemented, so large numbers of taxa are not able to be analyzed yet.
\item Data partitioning is not yet implemented. This is something that must be done before most potential users will be interested. I discussed ideas about how to implement this with Dave during the ATOL meeting here in March, but I haven't actually embarked on implementing it yet and we should probably talk about this some more as a group.
\item MCMCMC is not yet implemented. I suppose this is worthwhile, and should be done, but I must admit I am not as hot on this (ha ha) as I once was.
\item No way to summarize split posteriors or to draw trees. Given that this has been implemented before, it will not be hard to add, but thus far has not been incorporated. For the time being, I have been using PAUP to construct consensus trees.
\item No GUI. This may or may not be viewed as a disadvantage. I think a lot of labs will appreciate the ability to roll their own scripts using Phycas as a library, and I think the bioinformatics community will embrace Phycas as a tool they can simply add to their pipeline, so maybe this is the niche we should be shooting for, at least for the short term.
\item I'm sure other drawbacks will become apparent during your visit!
\end{itemize}

Here is a comparison of Phycas (SVN revision 41) with MrBayes 3.1.1 on two different data sets: nyldna4.nex (4 taxa, 3080 sites) and green.nex (10 taxa, 1296 sites). MrBayes devotes approximately 10\% of its likelihood evaluations to updating the parameters of the HKY model used (i.e. updating kappa and the four base frequencies). The other 90\% of its likelihood evaluations go toward updating the topology and edge lengths using LOCAL and extended TBR moves. I set up the Phycas runs to match both the total number of likelihood evaluations and this distribution of effort as closely as possible. {\em In summary, Phycas was about 8\% faster than MrBayes on the 4-taxon data set but MrBayes was about 55\% faster than Phycas on the 10-taxon data set}. This is actually surprisingly good, considering that I have not devoted any time yet to speeding up Phycas' likelihood calculation.  

\begin{tabular}{rrrrrrr}
            &             & Likelihood  &            & Evals.      & \% evals. spent & \% evals. spent \\
Data set    & Program     & evaluations &  Seconds   & per second  & on topology     & on parameters   \\ \hline
nyldna4.nex & Phycas (65) &  1009910    &   46.8     & 21593.2     &   89.1          & 10.9            \\
            & Phycas (95) &  1009910    &   60.4     & 16731.7     &   89.1          & 10.9            \\ 
            & MrBayes     &   999993    &   49.9     & 20056.4     &   90.9          &  9.1            \\ \hline
green.nex   & Phycas (65) &  1011178    &  311.7     & 3244.3      &   89.0          & 11.0            \\
            & Phycas (95) &  1011178    &  193.5     & 5225.6      &   89.0          & 11.0            \\
            & MrBayes     &   999993    &  199.1     & 5022.9      &   90.9          &  9.1            \\ \hline
\end{tabular}

\new\ The value in parentheses after the program name Phycas indicates the SVN revision number tested. Interestingly, Phycas became slower under the new CLA system for the 4-taxon case. When testing Phycas, the {\tt Phycas.py} script was run as is to generate the data. For future reference, here is a listing of the settings used in {\tt Phycas.py} for the above tests. Note that in revisions later than 95, the number of cycles was reduced from the 3000 used here to a much smaller value to speed up the testing cycle:

\begin{verbatim}
if __name__ == '__main__':
    mcmc = Phycas()

    mcmc.random_seed = '13579'
    mcmc.data_source = 'file'
    #mcmc.data_file_name = '../../pyphy/green.nex'
    mcmc.data_file_name = '../../pyphy/nyldna4.nex'
    mcmc.starting_tree_source = 'random'
    mcmc.ncycles = 3000
    mcmc.sample_every = 10
    mcmc.adapt_first = 10
    mcmc.model_type = 'hky'
    mcmc.ls_move_weight = 300
    mcmc.slice_max_units = 0
    mcmc.verbose = True

    mcmc.setup()
    mcmc.run()
\end{verbatim}

Below is a description of each user-modifiable setting. The default values are all defined in the Phycas constructor, \verb+Phycas.__init__()+.

%\subsection{XXXXXXXXXXX}
%\begin{description}
%\item[XXXXXXXXXXX] XXXXXXXXXXX
%\item[XXXXXXXXXXX] XXXXXXXXXXX
%\item[XXXXXXXXXXX] XXXXXXXXXXX
%\item[XXXXXXXXXXX] XXXXXXXXXXX
%\item[XXXXXXXXXXX] XXXXXXXXXXX
%\item[XXXXXXXXXXX] XXXXXXXXXXX
%\item[XXXXXXXXXXX] XXXXXXXXXXX
%\end{description}

\subsection{MCMC Settings}
\begin{description}
\item[{\tt random\_seed}] The master random number seed. If this is set to 'auto', a seed will be chosen from the system clock at the time the analysis begins
\item[{\tt ncycles}] The number of cycles through all parameters. Each cycle, every parameter will be updated \verb+slice_weight+ times in the order that they were added. Moves will be updated each cycle too, but the number of times moves are invoked depends on a move-dependent weight (e.g. \verb+ls_move_weight+ and \verb+ncat_move_weight+). This differs from the strategy used by MrBayes (in Phycas, the number of times moves or parameters are updated is deterministic rather than stochastic)
\item[{\tt sample\_every}] The number of cycles that must elapse before a sample is taken of all parameters
\item[{\tt report\_every}] The number of cycles that must elapse before progress is indicated to the user via the console
\item[{\tt verbose}] If True, much more output is sent to the console during a run (in fact, \verb+report_every+ is ignored unless \verb+verbose+ is True.
\end{description}

\subsection{Larget-Simon Moves}
\begin{description}
\item[{\tt ls\_move\_lambda}] The tuning parameter of the Larget-Simon move. Larger values lead to greater contraction/expansion of the selected 3-edge segment.
\item[{\tt ls\_move\_weight}] The number of times during a cycle in which the Larget-Simon move is attempted.
\item[{\tt ls\_move\_debug}] \new If True, a graphical tree viewer will pop up twice per update. The first time it will show the next proposed move and the second time it will show the results (after accepting or reverting the move).
\end{description}

\subsection{Bush (Polytomy) moves}
\begin{description}
\item[{\tt allow\_polytomies}] If True, add Bush move to the list of moves to be attempted during an MCMC analysis. If False, polytomous trees will not be visited during an MCMC analysis.
\item[{\tt polytomy\_prior}] If True, use the polytomy prior; if False, use the resolution class prior.
\item[{\tt topo\_prior\_C}] Specifies the strength of the prior ($C = 1$ is flat prior; $C > 1$ favors less resolved topologies)
\item[{\tt bush\_move\_edgelen\_mean}] Specifies mean of the exponential edge length generation distribution used by the Bush move when a new edges is created
\item[{\tt bush\_move\_weight}] Bush moves will be performed this many times per cycle if \verb+allow_polytomies+ is True
\item[{\tt bush\_move\_debug}] \new If True, a graphical tree viewer will pop up twice per update. The first time it will show the next proposed move and the second time it will show the results (after accepting or reverting the move).
\end{description}

\subsection{Slice Sampling}
\begin{description}
\item[{\tt slice\_weight}] Slice sampled parameters will be updated this many times per cycle.
\item[{\tt slice\_max\_units}] Maximum number of units used in slice sampling. If set to 0, the maximum number of units will be set to the largest possible value (i.e. \verb+UINT_MAX+)
\item[{\tt adapt\_first}] Adaptation of slice samplers is performed the first time at cycle \verb+adapt_first+. Subsequent adaptations wait twice the number of cycles as the previous adaptation. Thus, adaptation $n$ occurs at cycle \verb+adapt_first+ * ($2^n - 1$). The total number of adaptations that will occur during an MCMC run is [ln(\verb+adapt_first+ + \verb+ncycles+) - ln(\verb+adapt_first+)]/ln(2).
\item[{\tt adapt\_simple\_param}] Slice sampler adaptation parameter. The new width of a slice sampler unit will be \verb+adapt_simple_param+ times the average width of a slice (since the last time adaptation was performed)
\end{description}

\subsection{Edge Length Priors}
\begin{description}
\item[{\tt using\_hyperprior}] A hierarchical model for edge lengths will be used if True. In this case, the prior on edge lengths will be exponential with parameter $\mu$, where $\mu$ is a hyperparameter having a prior distribution determined by \verb+edgelen_hyperprior+. If this setting is False, the edge lengths will each have the prior distribution assigned to \verb+master_edgelen_dist+.
\item[{\tt edgelen\_hyperprior}] The hyperprior distribution. This can be any univariate probability distribution defined in the ProbDist module.
\item[{\tt master\_edgelen\_dist}] The edge length distribution.  This can be any univariate probability distribution defined in the ProbDist module.
\end{description}

\subsection{Substitution Model}
\begin{description}
\item[{\tt model\_type}] This can be 'jc', 'hky' or 'gtr' to select the JC, HKY or GTR models, respectively. This is deprecated because soon the model will be determined automatically from the parameters that have been added.
\item[{\tt num\_rates}] The number of relative rate categories used for discrete among-site rate heterogeneity. If \verb+use_flex_model+ is True, this determines only the starting number of relative rate categories.
\item[{\tt relrate\_prior}] This is the prior on unnormalized relative rate parameters that appear inside instantaneous rate matrices (not among-site relative rates). For example, the $\kappa$ transition/transversion rate ratio parameter in the HKY model uses this prior, as do the 6 unnormalized relative rate parameters used in the GTR model. This can be any univariate probability distribution defined in the ProbDist module.
\item[{\tt base\_freq\_param\_prior}] This is the prior on the unnormalized base frequencies. It can be any univariate probability distribution defined in the ProbDist module.
\item[{\tt gamma\_shape\_prior}] This is the prior on the gamma shape parameter determining the among-site rate heterogeneity when \verb+use_flex_model+ is False. It can be any univariate probability distribution defined in the ProbDist module.
\item[{\tt pinvar\_prior}] This is the prior used for the pinvar parameter in an invariable sites model. It can be any univariate probability distribution defined in the ProbDist module.
\item[{\tt use\_inverse\_shape}] If True, the prior used with discrete gamma rate heterogeneity will apply to $1/\alpha$ (i.e. the variance of relative rates) rather than $\alpha$ (the shape parameter of the underlying gamma distribution). One may wish to specify \verb+use_inverse_shape+ when the amount of rate heterogeneity is expected to be low (e.g. in I+G models where the invariable sites component of the model accounts for most of the rate heterogeneity).
\item[{\tt estimate\_pinvar}] If True, an invariable sites model will be used. If False, the pinvar parameter will be omitted.
\end{description}

\subsection{Data Source}
\begin{description}
\item[{\tt data\_source}] If set to None, causes MCMC to explore prior (i.e. MCMC is run with no data). If set to 'file', data\_file\_name should be a valid nexus file name. If set to 'memory', data is expected to already be resident in memory (e.g. data has been simulated)
\item[{\tt data\_file\_name}] Holds the path to the (Nexus-formatted) data file
\end{description}

\subsection{Tree Source}
\begin{description}
\item[{\tt starting\_tree\_source}] Source of starting tree topology. Can be either 'random', 'file' or 'usertree'. If 'random' is specified, a randomly-generated topology will be used as the starting tree. If 'file' is specified, the first tree topology defined in the data file will be used as the starting tree. If 'usertree' is specified, a string representing a Newick-style tree definition must be assigned to the variable \verb+tree_topology+.
\item[{\tt tree\_topology}] unused unless \verb+starting_tree_source+ is 'usertree'
\end{description}

\subsection{Gelfand-Ghosh (GG) Model Selection}
\begin{description}
\item[{\tt gg\_do}] If True, gather GG statistics during MCMC run
\item[{\tt gg\_outfile}] File in which to save GG results (use None if you plan to summarize the results yourself)
\item[{\tt gg\_nreps}] The number of replicate posterior predictive data sets to simulate for every MCMC sample
\item[{\tt gg\_kvect}] Vector of k values to use when computing Gm and Dm
\item[{\tt gg\_save\_postpreds}] If True, all posterior predictive data sets will be saved
\item[{\tt gg\_postpred\_prefix}] Prefix to use for posterior predictive dataset filenames (only used if save\_postpreds is True)
\item[{\tt gg\_save\_spectra}] Adds all 256 counts for posterior predictive simulated data sets to a file named spectra.txt, with counts separated by tabs (only use for four-taxon problems)
\end{description}

\subsection{FLEXCAT Model}
\begin{description}
\item[{\tt use\_flex\_model}] If True, FLEXCAT model will be used instead of assuming rate homogeneity or using the discrete gamma model of rate heterogeneity
\item[{\tt flex\_ncat\_move\_weight}] Number of times each cycle to attempt a move that changes the number of rate categories
\item[{\tt flex\_num\_spacers}] Number of fake rates between each adjacent pair of real rates. The more spacers specified, the more homogeneous will be the distances between adjacent relative rates
\item[{\tt flex\_phi}] Probability of proposing a category-changing move in which the number of categories is incremented (as opposed to decremented)
\item[{\tt flex\_L}] Upper bound of interval used for unnormalized relative rate parameter values
\item[{\tt flex\_lambda}] Parameter of Poisson prior on the number of extra (i.e. more than 1) rate categories
\item[{\tt flex\_prob\_param\_prior}] The prior distribution of category probabilities (also used to choose the new category probability when the number of rate categories is incremented)
\end{description}

\section{First Release TODO}

I have been creating tagged releases as I have added features, and keeping track of what goes into each tagged release in the file {\tt changes.txt}. Now that we have switched to SVN, it is the revision number that is important for recreating past versions, but I have stuck with the same naming scheme for releases. The current version is \verb+rel_0_8_1+, which corresponds to SVN revision 33.

Here are the currently missing features that I think are most important for release \verb+rel_1_0_0+. These are features that would make Phycas competitive with MrBayes. The goal is not to compete with MrBayes, per se, but without these features I think it would be hard to convince most current MrBayes users that Phycas is worth the effort of learning. It would ideal to have the first release coincide with Woods Hole this year. I am particularly interested in having something out there that people can use for the polytomy model and for the Gelfand-Ghosh paper that I hope to send in this summer.

\begin{itemize}
\item Data partitioning needs to be implemented. This is probably the single most {\em difficult} remaining obstacle.
\item Documentation. A very effective quick start guide needs to be written that can handle 90\% of the questions people are going to have. This is probably the single most {\em important} remaining obstacle because, without good documentation, potential users will be frightened away by even the small amount of Python coding needed to run Phycas.
\item Speed should be made comparable to MrBayes. \new\ This was accomplished in the NESCENT meeting.
\item Phycas should be able to handle large numbers of taxa (underflow handling needs to be implemented).
\item Split and topology managers need to be added so that Phycas can produce posterior summaries people expect.
\end{itemize}

While adding ML estimation, heuristic searching, etc. are important, my view is that these can be postponed until \verb+rel_2_0_0+. Phycas already has several features that distinguish it (in a good way!) from MrBayes. Incorporation of the CIPRES communication libraries should be perhaps higher in the priority list than maximum likelihood analyses. I am happy to add CIPRES communication to the list above for the first major release if you guys agree. I am anxious to learn how to do this, and a trip to FSU for me later this summer to work specifically on this would be a good idea I think.

\section{Example Applications}

As I have added features, I have added example applications to test those features. These are located in the subdirectories under the directory {\tt phycasdev/pyphy/apps}. A script (Windows batch file and Unix shell script) is present in the {\tt apps} directory that runs all the examples and checks their output against reference output. I do not tag a new release as such in the {\tt changes.txt} file until this test completes successfully. This test suite exercises nearly all of the functionality of Phycas, but many of the examples are not very clean and need to be redone. For example, {\tt MCMCSimple.py} is really a precursor to {\tt Phycas.py}, written before I implmented the Larget-Simon move. This one has thus outlived its usefulness and should be replaced entirely. Others are too complicated and would drive potential users away rather than attract them to Phycas. I will rework these in the near future to make them as simple and understandable as possible.

\section{Python and C++ Source Code Documentation}

Most of the C++ source code I have written thus far is located in the directories {\tt conversions}, {\tt data\_matrix}, {\tt likelihood}, {\tt phylogeny}, {\tt prob\_dist} and {\tt read\_nexus}, all subdirectories of {\tt phycasdev/pyphy}. This C++ source code is nearly all documented in a style suitable for Doxygen, although I have not yet tried to compile all of it into web pages. I have tried to be good about always creating three files for each new class: an {\tt .hpp} header file, a {\tt .inl} ``inlined functions'' file containing the inlined functions, and a {\tt .cpp} source code file. There are still many cases where I didn't follow this rule, but I've been getting better about conforming to this recently. It is very tempting to just define the inlined functions in place in the headers, but I find that if I do this then very few of them end up being documented.

The {\tt *\_pymod.cpp} files themselves serve as nice, concise summaries of which C++ functions and classes have been made available in the Python modules. These files are: 
\begin{itemize}
\item[] {\tt conversions\_pymod.cpp} in the {\tt phycasdev/pyphy/conversions} directory
\item[] {\tt data\_matrix\_pymod.cpp} in the {\tt phycasdev/pyphy/data\_matrix} directory
\item[] {\tt likelihood\_pymod.cpp} in the {\tt phycasdev/pyphy/likelihood} directory
\item[] {\tt phylogeny\_pymod.cpp} in the {\tt phycasdev/pyphy/phylogeny} directory
\item[] {\tt probdist\_pymod.cpp} in the {\tt phycasdev/pyphy/prob\_dist} directory
\item[] {\tt read\_nexus\_pymod.cpp} in the {\tt phycasdev/pyphy/read\_nexus} directory
\end{itemize}

I found that it was desirable to have source code documentation for the Python classes and member functions that is different from that for the corresponding C++ classes and member functions. This is because the Python source code is more visible to (and important for) ordinary users. For example, a user could type the following at a Python prompt to get a very useful description of how to create and sample from an exponential distribution:

{\tiny
\begin{verbatim}
>>> from ProbDist import *
>>> help(ExponentialDist)
Help on class ExponentialDist in module ProbDist._ExponentialDist:

class ExponentialDist(ProbDist._ProbDist.ExponentialDistBase)
 |  Represents the univariate exponential probability distribution.
 |
 |  Notes:
 |    - an exponential distribution with hazard parameter h has mean 1/h
 |    - the mean equals the standard deviation in exponential distributions
 |    - an exponential distribution with hazard parameter h is equivalent to
 |      a gamma distribution having shape 1 and scale parameter 1/h
 |
 |  Method resolution order:
 |      ExponentialDist
 |      ProbDist._ProbDist.ExponentialDistBase
 |      ProbDist._ProbDist.GammaDistBase
 |      ProbDist._ProbDist.ProbabilityDistribution
 |      ProbDist._ProbDist.AdHocDensityBase
 |      Boost.Python.instance
 |      __builtin__.object
 |
 |  Methods defined here:
 |
 |  __init__(self, hazard)
 |      Specify the hazard parameter (i.e. inverse of the mean) when
 |      initializing an ExponentialDist object. e.g.,
 |
 |      >>> from ProbDist import *
 |      >>> b = ExponentialDist(2)
 |      >>> print b.getMean()
 |      0.5
 |
 |  __repr__(self)
 |      Returns a string that could be used to initialize another
 |      ExponentialDist object identical to this one. e.g.,
 |
 |      >>> from ProbDist import *
 |      >>> b = ExponentialDist(2)
 |      >>> print b.__repr__()
 |      ExponentialDist(2.00000)
 |
 |  __str__(self)
 |      Returns a string that could be used to initialize another
 |      ExponentialDist object identical to this one. e.g.,
 |
 |      >>> from ProbDist import *
 |      >>> b = ExponentialDist(2)
 |      >>> print b.__str__()
 |      ExponentialDist(2.00000)
 |
 |  getCDF(self, x)
 |      Evaluates the cumulative distribution function at the supplied value
 |      x. This is the integral of the probability density function from 0.0
 |      up to the value x.
 |
 |      >>> from ProbDist import *
 |      >>> b = ExponentialDist(2)
 |      >>> print b.getCDF(1.5)
 |      0.950212931632
 |
 |      The integral of the exponential density is 1 - exp(-lambda*x), where
 |      lambda = 2 and x = 1.5 in this case. The CDF is thus 1 - exp(-3),
 |      which equals 0.950212931632.
 |
 |  getDistName(self)
 |      Returns the string 'Exponential'
 |
 |  getLnPDF(self, x)
 |      Evaluates the probability density function at the supplied value x.
 |      Returns the natural logarithm of the density at x. e.g.,
 |
 |      >>> from ProbDist import *
 |      >>> b = ExponentialDist(2)
 |      >>> print b.getLnPDF(1.5)
 |      -2.30685281944
 |
 |      The density is lambda*exp(-lambda*x), where lambda=2 and x=1.5. The
 |      log density is thus -lambda*x + log(lambda) = -(2)(1.5) + log(2),
 |      which equals -2.30685281944.
 |
 |  getMean(self)
 |      Returns the mean of the distribution. This is the theoretical mean
 |      (i.e., it will not change if sample is called to generate samples
 |      from this distribution).
 |
 |      >>> from ProbDist import *
 |      >>> b = ExponentialDist(2)
 |      >>> print b.getMean()
 |      0.5
 |
 |  getRelativeLnPDF(self, x)
 |      Evaluates the relative probability density function at the supplied
 |      value x. Returns the natural logarithm of the relative density at x.
 |      Use this function if speed is important but normalization is not, say
 |      in MCMC calculations. Use getLnPDF instead if you need to have a
 |      correctly normalized density value (i.e. from a density function that
 |      integrates to 1.0)
 |
 |      >>> from ProbDist import *
 |      >>> b = ExponentialDist(2)
 |      >>> print b.getRelativeLnPDF(1.5)
 |      -3.0
 |
 |      The density is lambda*exp(-lambda*x), where lambda=2 and x=1.5. The
 |      log density is thus -lambda*x + log(lambda) = -(2)(1.5) + log(2).
 |      The second term is not needed if only the relative PDF is required
 |      because it does not contain x, so the return value is just the first
 |      term, -3.0.
 |
 |  getStdDev(self)
 |      Returns the standard deviation of the distribution. This is the
 |      theoretical standard deviation (i.e., it will not change if sample is
 |      called to generate samples from this distribution).
 |
 |      >>> from ProbDist import *
 |      >>> b = ExponentialDist(2)
 |      >>> print b.getStdDev()
 |      0.5
 |
 |  getVar(self)
 |      Returns the variance of the distribution. This is the theoretical
 |      variance (i.e., it will not change if sample is called to generate
 |      samples from this distribution).
 |
 |      >>> from ProbDist import *
 |      >>> b = ExponentialDist(2)
 |      >>> print b.getVar()
 |      0.25
 |
 |  isDiscrete(self)
 |      Always returns False because the exponential distribution is
 |      continuous.
 |
 |  resetLot(self)
 |      Resets the random number generator to point to the local Lot object.
 |      Because the local Lot object is used by default, this function need
 |      only be called if setLot has previously been called to specify an
 |      external random number generator.
 |
 |  sample(self)
 |      Draws a single sampled value from the exponential distribution
 |      specified by this ExponentialDist object. Python list comprehensions
 |      can be used to store many simulated samples for use in subsequent
 |      calculations.
 |
 |      >>> from ProbDist import *
 |      >>> b = ExponentialDist(2)
 |      >>> b.setSeed(97531)
 |      >>> print "%.12f" % b.sample()
 |      0.720509647019
 |      >>> print "%.12f" % b.sample()
 |      0.006044079697
 |      >>> print "%.12f" % b.sample()
 |      1.429544116814
 |
 |  setLot(self, lot)
 |      Substitutes a different random number generator to use when drawing
 |      samples. e.g.,
 |
 |      >>> g = Lot()
 |      >>> g.setSeed(1357)
 |      >>> d1 = ExponentialDist(3.0)
 |      >>> d2 = ExponentialDist(2.0)
 |      >>> d1.setLot(g)
 |      >>> d2.setLot(g)
 |      >>> print "%.12f" % d1.sample()
 |      0.003559060607
 |      >>> print "%.12f" % d2.sample()
 |      0.343362432309
 |
 |      In this example, both d1 and d2 use the same random number generator
 |      when their sample functions are called. Generating all random numbers
 |      from a single random number generator has the advantage of allowing
 |      an entire analysis to be recreated by simply specifying the original
 |      seed (here, 1357) to the master Lot object. If setLot is not used,
 |      each distribution object maintains its own random number generator,
 |      which is initialized using the system clock at the time the object is
 |      created. This behavior is convenient, but makes it very hard to
 |      replicate results.
 |
 |  setMeanAndVariance(self, mean, var)
 |      Sets the mean and variance of this distribution. This distribution is
 |      determined entirely by the mean, so the var argument is ignored. The
 |      reason this function requires both mean and variance is for
 |      compatibility with functions of the same name in other distributions.
 |
 |      >>> from ProbDist import *
 |      >>> b = ExponentialDist(2)
 |      >>> print b.getMean()
 |      0.5
 |      >>> print b.getVar()
 |      0.25
 |      >>> b.setMeanAndVariance(5, 0)
 |      >>> print b.getMean()
 |      5.0
 |      >>> print b.getVar()
 |      25.0
 |
 |  setSeed(self, seed)
 |      Initializes the random number generator of this distribution object
 |      using the supplied seed. Note that if you have called setLot before
 |      this point, calling setSeed is pointless because you have already
 |      replaced the random number generator for which you are setting the
 |      seed! If you have already called setLot, you probably want to call
 |      the setSeed function of that Lot ojbect. e.g.,
 |
 |      >>> from ProbDist import *
 |      >>> b = ExponentialDist(2)
 |      >>> b.setSeed(135)
 |      >>> print "%.12f" % b.sample()
 |      0.000528559199
 |      >>> print "%.12f" % b.sample()
 |      0.708585892848
 |      >>> b.setSeed(135)
 |      >>> print "%.12f" % b.sample()
 |      0.000528559199
 |
 |  ----------------------------------------------------------------------
 |  Data and other attributes inherited from ProbDist._ProbDist.ExponentialDistBase:
 |
 |  __instance_size__ = 56
 |
 |  ----------------------------------------------------------------------
 |  Data and other attributes inherited from Boost.Python.instance:
 |
 |  __dict__ = <dictproxy object>
 |
 |
 |  __new__ = <built-in method __new__ of Boost.Python.class object>
 |      T.__new__(S, ...) -> a new object with type S, a subtype of T
 |
 |  __weakref__ = <member '__weakref__' of 'Boost.Python.instance' objects...
\end{verbatim}
}

Most of the above ``help'' comes directly from the Python source code documentation. While is is possible to put such documentation in the {\tt *\_pymod.cpp} files, it is much easier for users I think to create a shadow Python class that wraps the corresponding C++ class. The drawback is that it is more work, but good documentation is absolutely essential. The hard core programmers who want to peer into the C++ code behind the Python functionality will encounter the Doxygen documentation, which duplicates a lot of what is in the Python shadow class documentation, but adds more C++ specific information and is directed more toward programmers rather than users.

The tiny Python examples sprinkled above can all be tested for accuracy by running a single Python script, {\tt phycasdev/pyphy/pyphy/doctestall.py}. Each module, in its {\tt \_\_init\_\_.py} file, defines a function named {\tt testExamples()}. This function is called for each module and this causes the Python {\tt doctest} facility to check all of the examples in all of the {\tt .py} files listed in the {\tt testExamples()} function.

\section{Directory Structure}

Here are the most important parts of the directory structure for the purposes of the discussion above. I have not listed everything, only those files and directories that are most relevant to this overview. {\tt phycasdev} is the directory into which a working copy from the SVN repository ({\tt \svnurl}) has been checked out.

\begin{verbatim}
phycasdev/pyphy/apps                  test suite, example applications
phycasdev/pyphy/conversions           Python/STL conversions (e.g. list <-> vector)
phycasdev/pyphy/data_matrix           C++ code for creating DataMatrix Python module
phycasdev/pyphy/likelihood            C++ code for creating Likelihood Python module
phycasdev/pyphy/phylogeny             C++ code for creating Phylogeny Python module
phycasdev/pyphy/prob_dist             C++ code for creating ProbDist Python module
phycasdev/pyphy/pyphy                 Python modules are all created here
phycasdev/pyphy/read_nexus            C++ code for creating ReadNexus Python module
phycasdev/pyphy/changes.txt           Lists changes associated with each "release"
phycasdev/pyphy/dojam                 script that runs bjam to create Python modules
phycasdev/pyphy/Jamrules              rules used by bjam
phycasdev/pyphy/Jamfile               list of directories to be processed by bjam
\end{verbatim}

Here are the most important directories and files within the {\tt phycasdev/pyphy/apps} directory.

\begin{verbatim}
phycasdev/pyphy/apps/ExplorePrior     explores the joint prior distribution
phycasdev/pyphy/apps/FixedParams      fixes the values of some parameters
phycasdev/pyphy/apps/GelfandGhosh     compares models using Gelfand-Ghosh
phycasdev/pyphy/apps/LikelihoodTest   computes likelihood under all models
phycasdev/pyphy/apps/MCMCSimple       uses individual edge length updaters
phycasdev/pyphy/apps/Phycas           the Phycas application
phycasdev/pyphy/apps/Polytomies       reversible-jump MCMC over polytomous trees
phycasdev/pyphy/apps/Simulator        simulates data using Phycas
phycasdev/pyphy/apps/runall.sh        runs all apps, checks reference output
\end{verbatim}

Here are the most important directories and files within the {\tt phycasdev/pyphy/pyphy} directory.

\begin{verbatim}
phycasdev/pyphy/Conversions           Python module for STL/Python container conversions
phycasdev/pyphy/DataMatrix            Python module wrapping CipresDataMatrixHelper.cpp 
phycasdev/pyphy/Likelihood            Python module providing models and MCMC updaters
phycasdev/pyphy/Phylogeny             Python module encapsulating trees and tree nodes
phycasdev/pyphy/ProbDist              Python module providing probability distributions
phycasdev/pyphy/ReadNexus             Python module providing Nexus file reading capability
phycasdev/pyphy/doctestall.py         tests all examples in Python source code documentation
phycasdev/pyphy/nyldna4.nex           example data file used in many examples
\end{verbatim}

%
% Figure "felsenstein"
%
%\clearpage
%\begin{figure}
%\centering
%\hfil\includegraphics[scale=0.7]{felsenzone.eps}\hfil
%\end{figure}

\section{Literature Cited}
\renewcommand{\bibsection}{}
\bibliography{devsummary}

\end{document}
